{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application: Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus\n",
    "dataset\n",
    "\n",
    "### document\n",
    "each datapoin, represented as a single text, is called a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
    "# !tar xzf data/aclImdb_v1.tar.gz --skip-old-files -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aclImdb\n",
      "├── test\n",
      "│   ├── neg\n",
      "│   └── pos\n",
      "└── train\n",
      "    ├── neg\n",
      "    ├── pos\n",
      "    └── unsup\n",
      "\n",
      "7 directories\n"
     ]
    }
   ],
   "source": [
    "! tree -dL 2 data/aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsup folder contains unlabeled data, which we won’t use, and thereforeremove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is ahelper function in scikit-learn to load files stored in such afolder structure, where each subfolder corresponds to a label, calledload_files ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[6]: \n",
      " b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]: \\n {}\".format(text_train[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that the review contains some HTML line breaks. While these are unlikely to have a large impact on ourmachine learning models, it is better to clean the data and remove thisformatting before we proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \" ) for doc in text_train] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was collected such that the positive class and the negativeclass balanced, so that there are as many positive as negative strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test dataset in the same manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bards_words = [\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the CountVectorizer consists of the tokenization of thetraining data and building of the vocabulary, which we can access as thevocabulary_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content: \n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format (len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content: \\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the bag-of-words representation for the training data, we call the transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse matrix is usedas most documents only contain a small subset of the words in thevocabulary, meaning most entries in the feature array are 0. Thinkabout how many different words might appear in a movie review comparedto all the words in the English language (which is what the vocabularymodels). Storing all those zeros would be prohibitive, and a waste ofmemory. To look at the actual content of the sparse matrix, we canconvert it to a “dense” NumPy array (that also stores all the 0entries) using the toarray method:\n",
    "\n",
    "We can see that the word counts for each word are either 0 or 1;neither of the two strings in bards_words contains a word twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words: \n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words: \\n {}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words for Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      " <25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train: \\n {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X_train , the bag-of-words representation of the trainingdata, is 25,000×74,849, indicating that the vocabulary contains 74,849entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features: \n",
      " ['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "Features 20010 to 20030: \n",
      " ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "Every 2000th feature: \n",
      " ['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names ()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features: \\n {}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030: \\n {}\".format(feature_names[20010 : 20030]))\n",
    "print(\"Every 2000th feature: \\n {}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional, sparse data like this,linear models like LogisticRegression often work best. Let’s start byevaluating LogisticRegression using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a mean cross-validation score of 88%, which indicatesreasonable performance for a balanced binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv = 5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that LogisticRegression has a regularization parameter, C , whichwe can tune via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C' : [ 0.001 , 0.01 , 0.1 , 1 , 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid , cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to cut back on these is to only use tokens thatappear in at least two documents (or at least five documents, and so on). A tokenthat appears only in a single document is unlikely to appear in the testset and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the min_df parameter\n",
    "\n",
    "By requiring at least five appearances of each token, we can bring downthe number of features to 27,271, as seen in the preceding output only about athird of the original features. Let’s look at some tokens again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = 5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly many fewer numbers, and some of the more obscure wordsor misspellings seem to have vanished. Let’s see how well our modelperforms by doing a grid search again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features: \n",
      " ['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107', '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120', '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030: \n",
      " ['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions', 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying', 'replays', 'replete', 'replica']\n",
      "Every 700th feature: \n",
      " ['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered', 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds', 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate', 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking', 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea', 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"First 50 features: \\n {}\".format(feature_names[: 50]))\n",
    "print(\"Features 20010 to 20030: \\n {}\".format(feature_names[20010 : 20030]))\n",
    "print(\"Every 700th feature: \\n {}\".format(feature_names[:: 700]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy of the grid search is still 89%,unchanged from before. We didn’t improve our model, but having fewerfeatures to deal with speeds up processing and throwing away uselessfeatures might make the model more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid , cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a language-specific list of stopwords, or discardingwords that appear too frequently. scikit-learn has a built-in list ofEnglish stopwords in the feature_extraction.text module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword: \n",
      " ['con', 'others', 'etc', 'wherever', 'two', 'bill', 'mostly', 'un', 'five', 'never', 'except', 'into', 'few', 'he', 'twelve', 'some', 'thence', 'themselves', 'somehow', 'can', 'through', 'sometimes', 'ltd', 'also', 'here', 'my', 'mine', 'those', 'there', 'nobody', 'everyone', 'empty']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword: \\n {}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 305 (27,271–26,966) fewer features in the dataset, whichmeans that most, but not all, of the stopwords appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words: \n",
      " <25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "\n",
    "vect = CountVectorizer(min_df = 5, stop_words = \"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words: \\n {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run thegrid search again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, tf–idf had no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:  0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline ( TfidfVectorizer ( min_df = 5 ), LogisticRegression ())\n",
    "param_grid = {'logisticregression__C' : [ 0.001 , 0.01 , 0.1 , 1 , 10 ]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score:  {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect which words tf–idf foundmost important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train type <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_train.shape (25000, 27271)\n",
      "X_train[0,0] 0.10926829914120458\n",
      "max_value type <class 'numpy.ndarray'>\n",
      "max_value.shape (27271,)\n",
      "sorted_by_tfidf[:5] [23668 10103 11968 16943 22554]\n",
      "feature_names .shape (27271,)\n",
      "Features with lowest tfidf: \n",
      " ['suplexes' 'gauche' 'hypocrites' 'oncoming' 'songwriting' 'galadriel'\n",
      " 'emerald' 'mclaughlin' 'sylvain' 'oversee' 'cataclysmic' 'pressuring'\n",
      " 'uphold' 'thieving' 'inconsiderate' 'ware' 'denim' 'reverting' 'booed'\n",
      " 'spacious']\n",
      "Features with highest tfidf: \n",
      " ['gliding' 'orientated' 'attained' ... 'scanners' 'steve' 'pokemon']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# transform the training dataset\n",
    "X_train = vectorizer.transform(text_train)\n",
    "print(\"X_train type\", type(X_train))\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_train[0,0]\", X_train[0,518])\n",
    "\n",
    "# find maximum value for each of the features over the dataset\n",
    "max_value = X_train.max(axis = 0).toarray().ravel() \n",
    "print(\"max_value type\", type(max_value))\n",
    "print(\"max_value.shape\", max_value.shape)\n",
    "\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "print(\"sorted_by_tfidf[:5]\", sorted_by_tfidf[:5])\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"feature_names .shape\", feature_names.shape)\n",
    "print(\"Features with lowest tfidf: \\n {}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n {}\".format(feature_names[sorted_by_tfidf[20:]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
