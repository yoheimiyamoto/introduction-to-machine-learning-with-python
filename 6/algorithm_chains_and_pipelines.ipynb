{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Selection with Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s code forsplitting the data, computing the minimum and maximum, scaling the data, and training the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# load and split the data\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n",
    "\n",
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "svm = SVC()\n",
    "# learn an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scale the test data and score the scaled data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Test score: {:.2f}\".format ( svm . score ( X_test_scaled , y_test ))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s say we want to find better parameters for SVC using GridSearchCV , as discussed in Chapter 5 . How should we go about doingthis? A naive approach might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.98\n",
      "Best parameters:  {'C': 1, 'gamma': 1}\n",
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for illustration purposes only, don't use this code!\n",
    "param_grid = {'C' : [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ], 'gamma': [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ]}\n",
    "grid = GridSearchCV(SVC(), param_grid = param_grid , cv = 5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Test set accuracy: {:.2f}\".format(grid.score(X_test_scaled, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get around this problem, the splitting of the dataset duringcross-validation should be done before doing any preprocessing . Anyprocess that extracts knowledge from the dataset should only ever belearned from the training portion of the dataset, and therefore be containedinside the cross-validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we created two steps: the first, called \"scaler\" , is an instance ofMinMaxScaler , and the second, called \"svm\" , is an instance of SVC . Now, we can fitthe pipeline, like any other scikit-learn estimator:\n",
    "\n",
    "Here, pipe.fit first calls fit on the first step (the scaler), then transforms the training data using the scaler, and finally fits the SVMwith the scaled data. To evaluate on the test data, we simply callpipe.score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the score method on the pipeline first transforms the testdata using the scaler, and then calls the score method on the SVMusing the scaled test data. As you can see, the result is identical tothe one we got from the code at the beginning of the chapter, when doing the transformations by hand.Using the pipeline, we reduced the code needed for our “preprocessing   classification” process. The main benefit of using the pipeline,however, is that we can now use this single estimator incross_val_score or GridSearchCV ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pipelines in Grid Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntaxto define a parameter grid for a pipeline is to specify for eachparameter the step name, followed by __ (a double underscore),followed by the parameter name. To search over the C parameter of SVC we therefore have to use \"svm__C\" as the key in the parametergrid dictionary, and similarly for gamma :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'svm__C': [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ], 'svm__gamma': [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.98\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 1, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, param_grid = param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating Information Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the way we created the dataset, there is no relation between thedata, X , and the target, y (they are independent), so it should not be possible to learn anything from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rnd = np.random.RandomState(seed = 0)\n",
    "X = rnd.normal(size = (100, 10000))\n",
    "y = rnd.normal(size = (100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the most informative of the 10,000 features using SelectPercentile feature selection, and then we evaluate a Ridge regressor using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_selected.shape: (100, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile , f_regression\n",
    "\n",
    "select = SelectPercentile(score_func = f_regression, percentile = 5).fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "\n",
    "print(\"X_selected.shape: {}\".format(X_selected.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy (cv only on ridge): 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(np.mean(cross_val_score(Ridge(), X_selected, y, cv = 5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy (cv only on ridge): 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(np.mean(cross_val_score(Ridge(), X_selected, y, cv = 5)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean R 2 computed by cross-validation is 0.91,indicating a very good model. This clearly cannot be right,\n",
    "\n",
    "Because we fit the featureselection outside of the cross-validation, it could find features thatare correlated both on the training and the test folds. The informationwe leaked from the test folds was very informative, leading to highlyunrealistic results. Let’s compare this to a proper cross-validationusing a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy (pipeline): -0.25\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func = f_regression, percentile = 5)), (\"ridge\", Ridge())])\n",
    "print(\"Cross-validation accuracy (pipeline): {:.2f}\".format(np.mean(cross_val_score(pipe, X, y, cv = 5)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The General Pipeline Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only requirement for estimators in a pipeline is that all but thelast step need to have a transform method, so they can produce a newrepresentation of the data that can be used in the next step. Internally, during the call to Pipeline.fit , the pipeline callsfit and then transform on each step in turn, 2 with the input given by the output of the transform method of the previous step. For the last step in the pipeline, justfit is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for name, estimator in self.steps[: 1]:\n",
    "        # iterate over all but the final step\n",
    "        # fit and transform the data\n",
    "        X_transformed = estimator.fit_transform(X_transformed, y)\n",
    "    # fit the last step\n",
    "    self.steps[1][1].fit(X_transformed, y)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X ):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[: 1]:\n",
    "        # iterate over all but the final step\n",
    "        # transform the data\n",
    "        X_transformed = step[1].transform(X_transformed)\n",
    "    # predict using the last step\n",
    "    return self.steps[1][1].predict(X_transformed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenient Pipeline Creation with make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenience function, make_pipeline , that will create apipeline for us and automatically name each step based on its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# standard syntax\n",
    "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C = 100))])\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), SVC(C = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline objects pipe_long and pipe_short do exactly the samething, but pipe_short has steps that were automatically named.We can see the names of the steps by looking at the steps attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps: \n",
      " [('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svc', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline steps: \\n {}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the step names are just lowercase versions of the class names. If multiple steps have\n",
    "the same class, a number is appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps: \n",
      " [('standardscaler-1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('standardscaler-2', StandardScaler(copy=True, with_mean=True, with_std=True))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components = 2), StandardScaler())\n",
    "print(\"Pipeline steps: \\n {}\".format(pipe.steps)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Step Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to access the steps in a pipeline is viathe named_steps attribute, which is a dictionary from the step names tothe estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "components.shape: (2, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# fit the pipeline defined before to the cancer dataset\n",
    "pipe.fit(cancer.data)\n",
    "# extract the first two principal components from the \"pca\" step\n",
    "components = pipe.named_steps[\"pca\"].components_\n",
    "print(\"components.shape: {}\".format(components.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Attributes in a Pipeline inside GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier in this chapter, one of the main reasons to use pipelines is fordoing grid searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we used the make_pipeline function, the name of theLogisticRegression step in the pipeline is the lowercased class name,logisticregression . To tune the parameter C , we therefore have tospecify a parameter grid for logisticregression__C :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'logisticregression__C' : [ 0.01 , 0.1 , 1 , 10 , 100 ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we split the cancer dataset into training and test sets, andfit a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 4)\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Chapter 5 we know that thebest model found by GridSearchCV , trained on all the training data, isstored in grid.best_estimator_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: \n",
      " Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator: \\n {}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This best_estimator_ in our case is a pipeline with two steps,standardscaler and logisticregression . To access thelogisticregression step, we can use the named_steps attribute of thepipeline, as explained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression step: \n",
      " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression step: \\n {}\".format(grid.best_estimator_.named_steps[\"logisticregression\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the trained LogisticRegression instance, we canaccess the coefficients (weights) associated with each input feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression coefficients: \n",
      " [[-0.38856355 -0.37529972 -0.37624793 -0.39649439 -0.11519359  0.01709608\n",
      "  -0.3550729  -0.38995414 -0.05780518  0.20879795 -0.49487753 -0.0036321\n",
      "  -0.37122718 -0.38337777 -0.04488715  0.19752816  0.00424822 -0.04857196\n",
      "   0.21023226  0.22444999 -0.54669761 -0.52542026 -0.49881157 -0.51451071\n",
      "  -0.39256847 -0.12293451 -0.38827425 -0.4169485  -0.32533663 -0.13926972]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression coefficients: \\n {}\".format(grid.best_estimator_.named_steps[\"logisticregression\" ].coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Searching Preprocessing Steps and Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines, we can encapsulate all the processing steps in our machinelearning workflow in a single scikit-learn estimator. Another benefitof doing this is that we can now adjust the parameters of thepreprocessing using the outcome of a supervised task like regression orclassification. In previous chapters, we used polynomial features on theboston dataset before applying the ridge regressor. Let’s model thatusing a pipeline instead. The pipeline contains three steps scaling thedata, computing polynomial features, and ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston ()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PolynomialFeatures(), Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know which degrees of polynomials to choose, or whether tochoose any polynomials or interactions at all? Ideally we want to selectthe degree parameter based on the outcome of the classification. Usingour pipeline, we can search over the degree parameter together withthe parameter alpha of Ridge . To do this, we define a param_grid that contains both, appropriately prefixed by the step names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'polynomialfeatures__degree': [1, 2, 3], 'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = { 'polynomialfeatures__degree': [1, 2, 3], 'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, cv = 5, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results produced by the cross-validation, we can see thatusing polynomials of degree two helps, but that degree-three polynomialsare much worse than either degree one or two. This is reflected in thebest parameters that were found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a19f07ac8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1), vmin = 0, cmap = \"viridis\")\n",
    "plt.xlabel(\"ridge__alpha\")\n",
    "plt.ylabel(\"polynomialfeatures__degree\")\n",
    "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
    "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])), param_grid['polynomialfeatures__degree'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\n",
      "Test-set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run a grid search without polynomial features for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without poly features: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YoheiMiyamoto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'ridge__alpha' : [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ]}\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Searching Which Model To Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('preprocessing', StandardScaler ()), ('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we wanted to skip a step in thepipeline (for example, because we don’t need preprocessing for theRandomForest ), we can set that step to None :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier': [ SVC ()],\n",
    "        'preprocessing': [ StandardScaler (), None ],\n",
    "        'classifier__gamma' : [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ],\n",
    "        'classifier__C' : [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ]\n",
    "    },\n",
    "    {\n",
    "        'classifier' : [ RandomForestClassifier ( n_estimators = 100 )],\n",
    "        'preprocessing' : [ None ],\n",
    "        'classifier__max_features' : [ 1 , 2 , 3 ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate and run the grid search as usual, here on thecancer dataset:\n",
    "\n",
    "The outcome of the grid search is that SVC with StandardScaler preprocessing, C=10 , and gamma=0.01 gave the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      " {'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'preprocessing': StandardScaler(copy=True, with_mean=True, with_std=True)} \n",
      " \n",
      "Best cross-validation score: 0.99\n",
      "Test-set score: 0.98\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params: \\n {} \\n \".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Redundant Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a large grid-search like the ones described earlier,the same steps are often used several times.\n",
    "\n",
    "This can be done with the memory parameter of Pipeline , which takes a joblib.Memory object or justa path to store the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('preprocessing', StandardScaler ()), ('classifier', SVC())], memory = \"cache_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
